{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "    \n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e86e4339-b0c8-475d-99ee-68574071ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish.\"\n",
    "\n",
    "system_prompt = \"You are an assistant that analyzes the contents of text which is python code\\\n",
    "and explain what this code does and why: \\\n",
    "yield from {book.get('author') for book in books if book.get('author').\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb77e95d-2a89-47c4-98b7-9c9fb93a641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that takes code as input\n",
    "def get_code_prompt():\n",
    "    \"\"\"\n",
    "    Asks the user for a code snippet and returns it formatted as a prompt for an LLM.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted prompt with the user's code.\n",
    "    \"\"\"\n",
    "    print(\"Enter your code (press Enter twice to finish):\")\n",
    "    lines = []\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            line = input()\n",
    "            if line == \"\":\n",
    "                break\n",
    "            lines.append(line)\n",
    "        except EOFError:\n",
    "            break  # Handle end of input (Ctrl+D in Unix, Ctrl+Z in Windows)\n",
    "    \n",
    "    code_snippet = \"\\n\".join(lines)\n",
    "    \n",
    "    prompt = f\"Analyze and explain the following code:\\n\\n```python\\n{code_snippet}\\n```\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53ac3d36-877f-4ed2-a793-b8dc574dc8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your code (press Enter twice to finish):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " import ollama  def summarize_text(text, model=\"mistral\"):     \"\"\"     Summarizes the given text using Ollama's API.          Args:         text (str): The text to summarize.         model (str): The Ollama model to use (default is \"mistral\").          Returns:         str: The summarized text.     \"\"\"     prompt = f\"Summarize the following text:\\n\\n{text}\"          response = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}])          return response['message']['content']  if __name__ == \"__main__\":     text_to_summarize = \"\"\"Your long text goes here...\"\"\"     summary = summarize_text(text_to_summarize)     print(\"Summary:\\n\", summary)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "user_prompt = get_code_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55082840-7bff-4e94-ae2e-00a323bf6510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for():\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": get_code_prompt()}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6f14b13-1e3a-4f27-9eac-673a1471cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_explain(messages_for()):\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model = MODEL_GPT,\n",
    "        messages = messages_for()\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57dc6680-7515-4ece-a7df-ca0288b1ebfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your code (press Enter twice to finish):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " import ollama  def summarize_text(text, model=\"mistral\"):     \"\"\"     Summarizes the given text using Ollama's API.          Args:         text (str): The text to summarize.         model (str): The Ollama model to use (default is \"mistral\").          Returns:         str: The summarized text.     \"\"\"     prompt = f\"Summarize the following text:\\n\\n{text}\"          response = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}])          return response['message']['content']  if __name__ == \"__main__\":     text_to_summarize = \"\"\"Your long text goes here...\"\"\"     summary = summarize_text(text_to_summarize)     print(\"Summary:\\n\", summary)\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The provided Python code defines a function to summarize text using an API from Ollama. Here\\'s a breakdown of the different components of the code:\\n\\n1. **Importing the Library**: \\n   ```python\\n   import ollama\\n   ```\\n   This line imports the `ollama` library, which is presumably a library that provides functionality for interacting with AI models (such as summarization).\\n\\n2. **Defining the Function**:\\n   ```python\\n   def summarize_text(text, model=\"mistral\"):\\n   ```\\n   This line defines a function named `summarize_text`, which takes two arguments:\\n   - `text`: a string that contains the text you want to summarize.\\n   - `model`: a string that specifies which AI model to use for summarization, with a default value set to `\"mistral\"`.\\n\\n3. **Docstring Explanation**:\\n   ```python\\n   \"\"\"     \\n   Summarizes the given text using Ollama\\'s API.     \\n   Args:         \\n       text (str): The text to summarize.         \\n       model (str): The Ollama model to use (default is \"mistral\").         \\n   Returns:         \\n       str: The summarized text.     \\n   \"\"\"\\n   ```\\n   This docstring provides a concise description of the function\\'s purpose, its parameters, and what it returns. It helps other developers understand how to use the function.\\n\\n4. **Creating the Prompt**:\\n   ```python\\n   prompt = f\"Summarize the following text:\\\\n\\\\n{text}\" \\n   ```\\n   This line creates a prompt string formatted to instruct the AI model to summarize the provided text. The inserted text appears after the instruction.\\n\\n5. **Interacting with the API**:\\n   ```python\\n   response = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}])\\n   ```\\n   This line calls the `chat` method from the `ollama` library, sending the prompt to the selected model. The `messages` argument is a list that contains a dictionary, which indicates the role (user) and the message content (the prompt). The `response` variable captures the output returned by the API.\\n\\n6. **Returning the Summary**:\\n   ```python\\n   return response[\\'message\\'][\\'content\\']\\n   ```\\n   This line returns the summarized text from the API response. It accesses the \\'content\\' field nested within a \\'message\\' dictionary in the response object to extract the actual summary.\\n\\n7. **Main Execution Block**:\\n   ```python\\n   if __name__ == \"__main__\":\\n       text_to_summarize = \"\"\"Your long text goes here...\"\"\"\\n       summary = summarize_text(text_to_summarize)\\n       print(\"Summary:\\\\n\", summary)\\n   ```\\n   This part checks if the script is being executed as the main program. If so:\\n   - It defines a string variable `text_to_summarize` with a placeholder for some long text.\\n   - Calls the `summarize_text` function with this text to obtain a summary.\\n   - Prints the summarized text to the console under the label \"Summary:\".\\n\\n### Summary of Functionality:\\nIn summary, this code serves the primary purpose of summarizing a longer text using a specific language model from the `ollama` library. It does so by preparing a prompt, sending it to the model, and then returning and printing the summarized content. The code structure allows for easy modification of the text to summarize and the choice of model, making it flexible for different use cases.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_explain(messages_for())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3e3cd7d-5702-4991-9d00-3b7d16c303b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your code (press Enter twice to finish):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " import ollama  def summarize_text(text, model=\"mistral\"):     \"\"\"     Summarizes the given text using Ollama's API.          Args:         text (str): The text to summarize.         model (str): The Ollama model to use (default is \"mistral\").          Returns:         str: The summarized text.     \"\"\"     prompt = f\"Summarize the following text:\\n\\n{text}\"          response = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}])          return response['message']['content']  if __name__ == \"__main__\":     text_to_summarize = \"\"\"Your long text goes here...\"\"\"     summary = summarize_text(text_to_summarize)     print(\"Summary:\\n\", summary)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "user_prompt = get_code_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "453ccd64-eb6c-4778-ac96-47997a12658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Python script that uses the Ollama API to summarize text. Here's a breakdown of the code:\n",
      "\n",
      "**Importing Ollama**\n",
      "\n",
      "The first line imports the `ollama` library, which is not a built-in Python library but rather a third-party service that provides a simple interface for interacting with various language models.\n",
      "\n",
      "**Defining the `summarize_text` function**\n",
      "\n",
      "The script defines a function named `summarize_text` that takes two arguments:\n",
      "\n",
      "1. `text`: The text to be summarized.\n",
      "2. `model`: The Ollama model to use (defaults to \"mistral\").\n",
      "\n",
      "The function does the following:\n",
      "\n",
      "1. Constructs a prompt message by concatenating a summary prompt with the input `text`. This prompt is sent as a user message to the Ollama API.\n",
      "\n",
      "```python\n",
      "prompt = f\"Summarize the following text:\\n\\n{text}\"\n",
      "```\n",
      "\n",
      "2. Uses the `ollama.chat` function to send a single user message to the selected Ollama model (`model`). The input `text` is passed as part of the prompt message.\n",
      "\n",
      "```python\n",
      "response = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
      "```\n",
      "\n",
      "3. Retrieves the response from the Ollama API and extracts the summarized text from it.\n",
      "\n",
      "```python\n",
      "return response['message']['content']\n",
      "```\n",
      "\n",
      "**Main program**\n",
      "\n",
      "The script contains a main program block that:\n",
      "\n",
      "1. Defines a sample text to be summarized.\n",
      "2. Calls the `summarize_text` function with this input text and prints the resulting summary.\n",
      "\n",
      "```python\n",
      "if __name__ == \"__main__\":\n",
      "    text_to_summarize = \"\"\"Your long text goes here...\"\"\"\n",
      "    summary = summarize_text(text_to_summarize)\n",
      "    print(\"Summary:\\n\", summary)\n",
      "```\n",
      "\n",
      "**Notes**\n",
      "\n",
      "* The script assumes that you have a valid Ollama account and are using the \"mistral\" model, which is a popular language model for summarization.\n",
      "* This script uses a simple interface to interact with the Ollama API. Depending on your specific requirements, you might need to modify the code or use more advanced features of the library.\n",
      "\n",
      "To make this code more robust and user-friendly, consider adding error handling for cases like:\n",
      "\n",
      "* Invalid input text\n",
      "* Unreachable or unavailable Ollama models\n",
      "* Response errors from the API\n",
      "\n",
      "You can also improve the prompt construction by allowing users to customize the summary prompt and model selection.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "# Get Llama 3.2 to answer\n",
    "response = ollama.chat(model=MODEL_LLAMA, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced1723-475d-4582-bdff-e31aa7e18ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
